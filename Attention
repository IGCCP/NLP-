链接：https://zhuanlan.zhihu.com/p/146760904；https://zhuanlan.zhihu.com/p/143807314
来自：深度学习自然语言处理公众号


*详解Attention、Seq2Seq与交互式匹配

大脑中的注意力机制主要是为了减少计算量（忽略掉不重要的信息），然而在计算机里的attention机制却无一例外地需要增加计算量。
这是因为，计算机并不知道一堆数据哪个会比较重要，所以它需要一个全局的扫描，从而加大了计算。
正因如此，有人觉得注意力机制实际上更像是查询机制。因此该文从查询机制的角度理解attention。

文章认为：Attention机制 ≈ 软查询
软查询的软体现在查询的过程，字典里所有的元素权重不再非0即1了，只要相似（而不是全等）就有参考价值。
形式化的表达软查询的过程就是similar(QK)*V （这实际上是所有attention的通用框架）
